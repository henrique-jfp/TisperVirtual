#!/usr/bin/env python3
"""
Sistema de Impress√£o Inteligente para P√°ginas Web (v2 - Otimizado)
Converte HTML em Markdown e extrai dados para revis√£o humana (Human-in-the-Loop).

Melhorias implementadas:
- Remo√ß√£o da etapa de PDF: Convers√£o direta HTML -> Markdown para performance.
- Padr√£o Singleton para Browser: Inst√¢ncia √∫nica do browser.
- Preparado para HITL: Sa√≠da inclui markdown edit√°vel e status "pending_review".
- Processamento Non-blocking: Tarefas de CPU (parsing) executadas em ThreadPoolExecutor.
"""

import asyncio
import json
import logging
import os
import re
import sys
import hashlib
import time
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from functools import wraps
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor

# Depend√™ncias externas
try:
    import html2text
    from playwright.async_api import async_playwright, Browser, Playwright
    from bs4 import BeautifulSoup
    from groq import Groq
    from dotenv import load_dotenv
except ImportError as e:
    print(f"Erro de importa√ß√£o: {e}")
    print("Instale as depend√™ncias: pip install html2text playwright beautifulsoup4 groq python-dotenv")
    sys.exit(1)

# Carregar vari√°veis de ambiente
load_dotenv()


# ==================== CONFIGURA√á√ÉO ====================

@dataclass
class Config:
    """Configura√ß√£o centralizada do sistema"""
    # API
    groq_api_key: str = os.getenv('LLM_API_KEY', '')
    groq_model: str = os.getenv('GROQ_MODEL', 'llama-3.1-8b-instant')
    
    # Timeouts e limites
    page_timeout: int = int(os.getenv('PAGE_TIMEOUT', '30000'))
    page_load_wait: int = int(os.getenv('PAGE_LOAD_WAIT', '3000'))

    # Aumentado para suportar markdown completo
    max_content_length: int = int(os.getenv('MAX_CONTENT_LENGTH', '15000')) 
    max_tokens: int = int(os.getenv('MAX_TOKENS', '4000'))
    
    # Retry
    max_retries: int = int(os.getenv('MAX_RETRIES', '3'))
    retry_delay: float = float(os.getenv('RETRY_DELAY', '2.0'))
    
    # Cache
    enable_cache: bool = os.getenv('ENABLE_CACHE', 'true').lower() == 'true'
    cache_dir: Path = Path(os.getenv('CACHE_DIR', './cache'))
    cache_ttl_hours: int = int(os.getenv('CACHE_TTL_HOURS', '24'))
    
    # Diret√≥rios
    output_dir: Path = Path(os.getenv('OUTPUT_DIR', './output'))
    log_dir: Path = Path(os.getenv('LOG_DIR', './logs'))
    
    # Viewport
    viewport_width: int = 1200
    viewport_height: int = 800
    
    def __post_init__(self):
        """Criar diret√≥rios necess√°rios"""
        for directory in [self.cache_dir, self.output_dir, self.log_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def validate(self) -> Tuple[bool, List[str]]:
        """Valida a configura√ß√£o"""
        errors = []
        if not self.groq_api_key:
            errors.append("GROQ_API_KEY n√£o configurada")
        return len(errors) == 0, errors


# ==================== ENUMS ====================

class ContentType(Enum):
    """Tipos de conte√∫do suportados"""
    MATCH_PREVIEW = "match_preview"
    PLAYER_NEWS = "player_news"
    MATCH_REPORT = "match_report"
    INJURY_REPORT = "injury_report"
    GAME_STATS = "game_stats"
    GENERAL_NEWS = "general_news"
    ERROR = "error"


class ProcessingStatus(Enum):
    """Status do processamento"""
    PENDING_REVIEW = "pending_review"  # Status para HITL (Human-in-the-Loop)
    COMPLETED = "completed"
    FAILED = "failed"
    CACHED = "cached"


# ==================== LOGGING ====================

class LoggerSetup:
    @staticmethod
    def setup(config: Config) -> logging.Logger:
        logger = logging.getLogger('SmartWebPrinter')
        if logger.hasHandlers():
            return logger
            
        logger.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s')
        
        log_file = config.log_dir / f'smart_printer_{datetime.now():%Y%m%d}.log'
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        
        console_handler = logging.StreamHandler(sys.stderr)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(logging.Formatter('%(levelname)s | %(message)s'))
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        return logger


# ==================== UTILIT√ÅRIOS ====================

class CacheManager:
    def __init__(self, cache_dir: Path, ttl_hours: int = 24):
        self.cache_dir = cache_dir
        self.ttl_hours = ttl_hours
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_cache_key(self, url: str) -> str:
        return hashlib.sha256(url.encode()).hexdigest()

    def _get_cache_path(self, cache_key: str) -> Path:
        return self.cache_dir / f"{cache_key}.json"

    def get(self, url: str) -> Optional[Dict[str, Any]]:
        cache_path = self._get_cache_path(self._get_cache_key(url))
        if not cache_path.exists():
            return None
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            cached_time = datetime.fromisoformat(cached_data.get('cached_at', '2000-01-01'))
            if (datetime.now() - cached_time).total_seconds() / 3600 > self.ttl_hours:
                cache_path.unlink()
                return None
                
            return cached_data.get('data')
        except Exception:
            return None

    def set(self, url: str, data: Dict[str, Any]):
        cache_path = self._get_cache_path(self._get_cache_key(url))
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'url': url,
                    'cached_at': datetime.now().isoformat(),
                    'data': data
                }, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

def retry_with_backoff(max_retries: int = 3, initial_delay: float = 1.0):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            delay = initial_delay
            last_exception = None
            for _ in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    await asyncio.sleep(delay)
                    delay *= 2
            raise last_exception
        return wrapper
    return decorator


# ==================== CLASSE PRINCIPAL ====================

class SmartWebPrinter:
    """
    Sistema de impress√£o inteligente de p√°ginas web.
    Gerencia ciclo de vida do browser (Singleton) e processamento paralelo.
    """
    
    AD_SELECTORS = [
        '[id*="google_ads"]', '[class*="google-ads"]', '[id*="adsbygoogle"]',
        '[class*="advertisement"]', '[class*="ads-banner"]', '[class*="sponsored"]',
        '[id*="banner"]', '[class*="popup"]', '[class*="modal"]',
        '[class*="social-share"]', 'nav', 'header', 'footer', 'aside', '[class*="comment"]',
        'script', 'style', 'iframe'
    ]

    CONTENT_PROMPTS = {
        ContentType.MATCH_PREVIEW: "Analise esta pr√©-visualiza√ß√£o de jogo e extraia:\n- Times envolvidos (home e away)\n- Data e hor√°rio (ISO 8601)\n- Competi√ß√£o\n- An√°lise t√°tica\n- Jogadores em destaque\n- Probabilidades",
        ContentType.PLAYER_NEWS: "Analise esta not√≠cia sobre jogador e extraia:\n- Nome do jogador\n- Time atual\n- Tipo de not√≠cia (les√£o, transfer√™ncia, etc.)\n- Detalhes espec√≠ficos\n- Impacto no time",
        ContentType.MATCH_REPORT: "Analise este relat√≥rio de jogo e extraia:\n- Times (home e away)\n- Placar final\n- Gols (jogador, minuto)\n- Estat√≠sticas principais\n- Destaques\n- Data (ISO 8601)",
        ContentType.INJURY_REPORT: "Analise este relat√≥rio de les√µes e extraia:\n- Jogadores lesionados\n- Tipo de les√£o\n- Gravidade\n- Tempo de recupera√ß√£o\n- Status atual",
        ContentType.GAME_STATS: "Analise estas estat√≠sticas de jogo e extraia:\n- Times (home e away)\n- Data (ISO 8601)\n- Placar final\n- Estat√≠sticas detalhadas (Posse, Chutes, Corners, etc.) em formato {'stat': {'home': 'valor', 'away': 'valor'}}",
        ContentType.GENERAL_NEWS: "Analise esta not√≠cia geral de futebol e extraia:\n- T√≥pico principal\n- Times e jogadores mencionados\n- Competi√ß√£o\n- Informa√ß√µes relevantes"
    }

    def __init__(self, config: Optional[Config] = None):
        self.config = config or Config()
        is_valid, errors = self.config.validate()
        if not is_valid:
            raise ValueError(f"Configura√ß√£o inv√°lida: {', '.join(errors)}")
        
        self.logger = LoggerSetup.setup(self.config)
        self.groq_client = Groq(api_key=self.config.groq_api_key)
        self.cache = CacheManager(self.config.cache_dir, self.config.cache_ttl_hours)
        
        # Singleton Resources
        self.playwright: Optional[Playwright] = None
        self.browser: Optional[Browser] = None
        
        # Executor para tarefas CPU-bound (HTML parsing/conversion)
        self.executor = ThreadPoolExecutor(max_workers=min(4, os.cpu_count() or 1))
        self.logger.info("SmartWebPrinter inicializado")

    async def __aenter__(self):
        """Inicializa o browser (padr√£o Singleton)"""
        if self.browser is None:
            self.logger.info("Iniciando inst√¢ncia Singleton do browser...")
            self.playwright = await async_playwright().__aenter__()
            self.browser = await self.playwright.chromium.launch(headless=True)
            self.logger.info("Browser iniciado com sucesso.")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Fecha o browser e desliga o executor"""
        if self.browser:
            self.logger.info("Fechando inst√¢ncia do browser...")
            await self.browser.close()
        if self.playwright:
            await self.playwright.__aexit__(exc_type, exc_val, exc_tb)
        
        self.executor.shutdown(wait=True)
        self.logger.info("Recursos liberados.")

    def _convert_html_to_markdown(self, html_content: str) -> str:
        """
        [CPU-BOUND] Limpa o HTML com BeautifulSoup e converte para Markdown.
        Executada em um ThreadPoolExecutor para n√£o bloquear o loop.
        """
        try:
            # 1. Limpeza com BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remover elementos indesejados
            for selector in self.AD_SELECTORS:
                for element in soup.select(selector):
                    element.decompose()

            # 2. Convers√£o para Markdown
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = True
            h.body_width = 0  # 0 = sem quebra de linha for√ßada
            h.protect_links = True
            
            markdown = h.handle(str(soup))

            # 3. Limpeza final do Markdown (linhas vazias excessivas)
            lines = [line.strip() for line in markdown.split('\n')]
            clean_markdown = '\n'.join(line for line in lines if line)
            
            return clean_markdown
        except Exception as e:
            self.logger.error(f"Erro na convers√£o HTML->Markdown: {e}")
            return ""

    @retry_with_backoff(max_retries=3, initial_delay=2.0)
    async def _get_page_html(self, url: str) -> str:
        """Busca o conte√∫do HTML de uma p√°gina com retry usando o browser singleton."""
        if not self.browser:
            raise RuntimeError("Browser n√£o inicializado. Use 'async with SmartWebPrinter()'.")

        context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': self.config.viewport_width, 'height': self.config.viewport_height}
        )
        
        page = await context.new_page()
        try:
            self.logger.debug(f"Navegando para: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=self.config.page_timeout)
            
            # Pequena espera para hidrata√ß√£o de frameworks JS (React/Next/Vue)
            await page.wait_for_timeout(self.config.page_load_wait)
            
            html_content = await page.content()
            return html_content
        except Exception as e:
            self.logger.error(f"Erro ao carregar p√°gina {url}: {e}")
            raise e
        finally:
            await context.close()

    def _detect_content_type(self, url: str, content: str) -> ContentType:
        url_lower = url.lower()
        content_lower = content.lower()[:2000]
        
        keywords = {
            ContentType.MATCH_PREVIEW: {'url': ['preview', 'previsao'], 'content': ['pr√≥ximo jogo', 'pr√©via', 'prov√°vel escala√ß√£o']},
            ContentType.INJURY_REPORT: {'url': ['injury', 'lesao'], 'content': ['lesionado', 'departamento m√©dico', 'desfalque']},
            ContentType.MATCH_REPORT: {'url': ['report', 'resumo', 'pos-jogo'], 'content': ['placar final', 'gols', 'melhores momentos']},
            ContentType.GAME_STATS: {'url': ['stats', 'estatisticas'], 'content': ['posse de bola', 'chutes a gol', 'escanteios']},
            ContentType.PLAYER_NEWS: {'url': ['transfer', 'contratacao', 'mercado'], 'content': ['transfer√™ncia', 'contrato', 'assinou']}
        }
        
        for c_type, kw in keywords.items():
            if any(w in url_lower for w in kw['url']) or any(w in content_lower for w in kw['content']):
                return c_type
        return ContentType.GENERAL_NEWS

    def _extract_link_info(self, structured_data: Dict[str, Any], url: str) -> Dict[str, Any]:
        """Gera metadados auxiliares para links"""
        link_info = {
            'teams_found': structured_data.get('teams', []),
            'players_found': structured_data.get('players', []),
            'date_found': structured_data.get('date'),
            'game_candidates': []
        }
        
        if structured_data.get('content_type') in ['match_report', 'game_stats', 'match_preview'] and len(link_info['teams_found']) >= 2:
            link_info['game_candidates'].append({
                'home_team': link_info['teams_found'][0],
                'away_team': link_info['teams_found'][1],
                'date': link_info['date_found'],
                'url': url
            })
        return link_info

    async def extract_structured_data(self, markdown_content: str, url: str) -> Dict[str, Any]:
        """Extrai dados estruturados usando IA via Groq."""
        try:
            self.logger.info("Analisando conte√∫do Markdown com IA...")
            content_type = self._detect_content_type(url, markdown_content)
            
            # Truncar conte√∫do para n√£o estourar contexto do LLM
            content_excerpt = markdown_content[:self.config.max_content_length]
            
            specific_prompt = self.CONTENT_PROMPTS.get(content_type, self.CONTENT_PROMPTS[ContentType.GENERAL_NEWS])

            full_prompt = f"""Analise o conte√∫do Markdown a seguir e extraia informa√ß√µes estruturadas.
URL: {url}
Tipo Detectado: {content_type.value}
{specific_prompt}

Conte√∫do (Markdown):
{content_excerpt}

INSTRU√á√ïES CR√çTICAS: 
1. Responda APENAS com JSON v√°lido.
2. N√£o use blocos de c√≥digo markdown (```json).
3. Use formato ISO 8601 para datas.
4. Se houver d√∫vida, marque 'confidence' baixo.

Formato de resposta esperado:
{{
    "content_type": "{content_type.value}", 
    "title": "T√≠tulo Sugerido", 
    "teams": ["Time A", "Time B"], 
    "players": ["Jogador 1"],
    "date": "YYYY-MM-DDTHH:MM:SS", 
    "competition": "Nome da Competi√ß√£o", 
    "data": {{ "campo_especifico": "valor" }}, 
    "confidence": 0.85
}}"""

            response = await asyncio.to_thread(
                self.groq_client.chat.completions.create,
                model=self.config.groq_model,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.1,
                max_tokens=self.config.max_tokens
            )
            
            result = response.choices[0].message.content.strip()
            # Limpeza de seguran√ßa para JSON
            result = re.sub(r'^```(?:json)?\s*|\s*```$', '', result).strip()
            
            structured_data = json.loads(result)
            structured_data.update({
                'source_url': url,
                'extracted_at': datetime.now().isoformat(),
                'link_info': self._extract_link_info(structured_data, url)
            })
            
            self.logger.info(f"Dados extra√≠dos (confian√ßa: {structured_data.get('confidence', 0):.2f})")
            return structured_data

        except Exception as e:
            self.logger.error(f"Erro na extra√ß√£o com IA: {e}", exc_info=True)
            return self._create_error_result(url, str(e))

    def _create_error_result(self, url: str, error: str) -> Dict[str, Any]:
        return {
            'content_type': ContentType.ERROR.value,
            'status': ProcessingStatus.FAILED.value,
            'error': error,
            'source_url': url,
            'extracted_at': datetime.now().isoformat()
        }

    async def process_web_content(self, url: str, force: bool = False) -> Optional[Dict[str, Any]]:
        """
        Processo completo: URL -> HTML -> Markdown -> Dados Estruturados.
        Retorna objeto pronto para revis√£o humana (HITL).
        """
        try:
            self.logger.info(f"Iniciando processamento: {url}")
            if not urlparse(url).scheme:
                return self._create_error_result(url, "URL inv√°lida")

            # 1. Verificar Cache
            if self.config.enable_cache and not force:
                if cached := self.cache.get(url):
                    self.logger.info("Resultado recuperado do cache")
                    return cached

            # 2. Obter HTML (I/O Bound)
            raw_html = await self._get_page_html(url)
            if not raw_html:
                return self._create_error_result(url, "Falha ao obter HTML")

            # 3. Converter para Markdown (CPU Bound - ThreadPool)
            loop = asyncio.get_running_loop()
            editable_markdown = await loop.run_in_executor(
                self.executor, 
                self._convert_html_to_markdown, 
                raw_html
            )
            
            if not editable_markdown:
                self.logger.warning("Markdown gerado est√° vazio.")

            self.logger.info(f"Markdown gerado com {len(editable_markdown)} caracteres.")

            # 4. Extrair Dados com IA (I/O Bound)
            structured_data = await self.extract_structured_data(editable_markdown, url)

            # 5. Preparar para HITL (Human-in-the-Loop)
            # Adicionamos o markdown cru para edi√ß√£o no frontend e definimos status
            structured_data['status'] = ProcessingStatus.PENDING_REVIEW.value
            structured_data['editable_markdown'] = editable_markdown
            
            # Salvar no cache
            if self.config.enable_cache:
                self.cache.set(url, structured_data)

            self.logger.info("‚úÖ Processamento conclu√≠do, pronto para revis√£o.")
            return structured_data

        except Exception as e:
            self.logger.error(f"Erro no processamento: {e}", exc_info=True)
            return self._create_error_result(url, str(e))


# ==================== CLI ====================

class CLI:
    async def run(self, args: List[str]):
        if len(args) < 2:
            self.print_usage()
            return

        try:
            # Context Manager garante que o browser abra uma vez e feche ao final
            async with SmartWebPrinter(Config()) as printer:
                command = args[1]
                
                if command == 'process':
                    await self.cmd_process(printer, args[2:])
                else:
                    print(f"‚ùå Comando desconhecido: {command}")
                    self.print_usage()
                    
        except Exception as e:
            print(f"‚ùå Erro fatal na execu√ß√£o: {e}")

    async def cmd_process(self, printer: SmartWebPrinter, args: List[str]):
        if not args:
            print("‚ùå URL n√£o fornecida")
            return
            
        url = args[0]
        force = '--force' in args

        print(f"\nüöÄ Processando: {url}")
        result = await printer.process_web_content(url, force=force)

        if result:
            # Sa√≠da JSON pura para integra√ß√£o com outros sistemas
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            print(json.dumps({"error": "Falha desconhecida"}, ensure_ascii=False))

    def print_usage(self):
        print("\nUSO: python smart_web_printer.py process <URL> [--force]\n")


# ==================== MAIN ====================

async def main():
    cli = CLI()
    await cli.run(sys.argv)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro n√£o tratado: {e}")